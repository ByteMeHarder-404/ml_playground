{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "BE-vKUV-N5rC"
      },
      "outputs": [],
      "source": [
        "from PIL import Image,ImageEnhance,ImageFilter\n",
        "import torch\n",
        "from torchvision import transforms,models\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "LQZ9s_xej0mZ"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean=[0.485,0.456,0.406]\n",
        "std=[0.229,0.224,0.225]"
      ],
      "metadata": {
        "id": "KEXRrNffqvK2"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean,std)\n",
        "])"
      ],
      "metadata": {
        "id": "goom_k64q65Z"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def depreprocess(tensor):\n",
        "  t=tensor.clone()\n",
        "  for c in range(3):\n",
        "    t[c]=t[c]*std[c]+mean[c]\n",
        "  return torch.clamp(t, 0.0, 1)"
      ],
      "metadata": {
        "id": "ZEZUK9O8rYHR"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_to_pil(t):\n",
        "  t = t.squeeze(0).cpu().clone()\n",
        "  t = depreprocess(t)\n",
        "  return transforms.ToPILImage()(t)"
      ],
      "metadata": {
        "id": "z9sqwXB-2C89"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pth='/content/Screenshot 2025-10-15 160131.png'\n",
        "img=Image.open(pth).convert('RGB')\n",
        "timg=pre(img).unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "Bz47BkaSr2jy"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timg.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iZlN7QgsiNM",
        "outputId": "b4c7f67f-8750-4cc4-ce5d-994180db2b0e"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 89, 93])"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=models.vgg16(pretrained=True).to(device).eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uUMsfSms9dS",
        "outputId": "8dbaa959-ac93-4c4e-cb71-f4f702388984"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad=False"
      ],
      "metadata": {
        "id": "8swrBR0gtqrC"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lay=[8,15,22,29]\n",
        "wei=[0.5,1,2,4]"
      ],
      "metadata": {
        "id": "WLjVR7HIt6Cn"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Extract(nn.Module):\n",
        "  def __init__(self,feat,lay):\n",
        "    super().__init__()\n",
        "    self.feat=feat\n",
        "    self.lay=set(lay)\n",
        "  def forward(self,x):\n",
        "    acti=[]\n",
        "    for i,lays in enumerate(self.feat):\n",
        "      x=lays(x)\n",
        "      if i in self.lay:\n",
        "        acti.append(x)\n",
        "    return acti"
      ],
      "metadata": {
        "id": "-O8PJBD1uBrA"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ext=Extract(model.features,lay).to(device)"
      ],
      "metadata": {
        "id": "59Kaf-t1vmZa"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def losss(acti,wei=None):\n",
        "  if wei is None:\n",
        "    wei=[1.0]*len(acti)\n",
        "  loss=0.0\n",
        "  for act,w in zip(acti,wei):\n",
        "    loss+=w*act.pow(2).mean()\n",
        "  return loss"
      ],
      "metadata": {
        "id": "RPmyUB5Xvz3C"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "rZfThV1WzAkW"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def asc(timg,lr=0.09,jitter=12):\n",
        "  ox,oy=np.random.randint(-jitter,jitter+1,2)\n",
        "  timg.data=torch.roll(timg.data,shifts=(ox,oy),dims=(2,3))\n",
        "  activ=ext(timg)\n",
        "  loss=-losss(activ,wei)\n",
        "  timg.grad=None\n",
        "  loss.backward()\n",
        "  grad=timg.grad.data\n",
        "  timg.data+=lr*grad/(grad.std()+1e-8)\n",
        "  timg.data=torch.roll(timg.data,shifts=(-ox,-oy),dims=(2,3))\n",
        "  timg.data=torch.clamp(timg.data,-2.2,2.6)\n",
        "  return loss.item()"
      ],
      "metadata": {
        "id": "b7jo-5WWzf0v"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def oct(imgpil,octa=4,octscale=1.5,itr=80):\n",
        "  octimg=[imgpil]\n",
        "  for i in range(1,octa):\n",
        "    nsize=(int(imgpil.width/(octscale**i)),int(imgpil.height/(octscale**i)))\n",
        "    if nsize[0]<64 or nsize[1]<64:\n",
        "      break\n",
        "    octimg.append(imgpil.resize(nsize,Image.LANCZOS))\n",
        "  octimg=octimg[::-1]\n",
        "  det=None\n",
        "  for octas in octimg:\n",
        "    if det is None:\n",
        "      cur=octas\n",
        "    else:\n",
        "      det=det.resize(octas.size,Image.LANCZOS)\n",
        "      cur=Image.blend(octas,det,alpha=0.5)\n",
        "    imgten=pre(cur).unsqueeze(0).to(device)\n",
        "    imgten.requires_grad_(True)\n",
        "    for i in range(itr):\n",
        "      asc(imgten)\n",
        "    det=tensor_to_pil(imgten)\n",
        "  return det"
      ],
      "metadata": {
        "id": "99Tl-qJm0wlA"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_creepier(img):\n",
        "    # Sharpen FIRST before other effects\n",
        "    img = img.filter(ImageFilter.UnsharpMask(radius=1.5, percent=120, threshold=3))\n",
        "\n",
        "    # Increase contrast for harsh, unsettling look\n",
        "    enhancer = ImageEnhance.Contrast(img)\n",
        "    img = enhancer.enhance(1.3)\n",
        "\n",
        "    # Slightly desaturate for uncanny valley effect\n",
        "    enhancer = ImageEnhance.Color(img)\n",
        "    img = enhancer.enhance(0.8)\n",
        "\n",
        "    # Sharpen again after color adjustments\n",
        "    img = img.filter(ImageFilter.SHARPEN)\n",
        "\n",
        "    # Darken slightly for ominous mood\n",
        "    enhancer = ImageEnhance.Brightness(img)\n",
        "    img = enhancer.enhance(0.92)\n",
        "\n",
        "    return img"
      ],
      "metadata": {
        "id": "902UtHkDKT3G"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final = make_creepier(oct(\n",
        "    imgpil=img,octa=4,octscale=1.4,itr=80\n",
        "))"
      ],
      "metadata": {
        "id": "8UyJTq5AyX6u"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final.save(\"deepdream_output.png\")"
      ],
      "metadata": {
        "id": "PHsT3MGh3x51"
      },
      "execution_count": 207,
      "outputs": []
    }
  ]
}