{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "cHZmn-IKeqye"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clanker=models.vgg19(pretrained=True).features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfVG3k08e7VP",
        "outputId": "f385690b-494b-447a-f342-9ff39d037ee7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clanker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzoNjTu5fm0G",
        "outputId": "6f13fe94-70da-4a74-c7dd-428fbbcdcf27"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (3): ReLU(inplace=True)\n",
              "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (6): ReLU(inplace=True)\n",
              "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (8): ReLU(inplace=True)\n",
              "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (11): ReLU(inplace=True)\n",
              "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (13): ReLU(inplace=True)\n",
              "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (15): ReLU(inplace=True)\n",
              "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (17): ReLU(inplace=True)\n",
              "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (20): ReLU(inplace=True)\n",
              "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (22): ReLU(inplace=True)\n",
              "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (24): ReLU(inplace=True)\n",
              "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (26): ReLU(inplace=True)\n",
              "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (29): ReLU(inplace=True)\n",
              "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (31): ReLU(inplace=True)\n",
              "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (33): ReLU(inplace=True)\n",
              "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (35): ReLU(inplace=True)\n",
              "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Vgg(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.features=['0','5','10','19','28']\n",
        "    self.model=models.vgg19(pretrained=True).features[:29]\n",
        "  def forward(self,x):\n",
        "    layers=[]\n",
        "    for no,layer in enumerate(self.model):\n",
        "      x=layer(x)\n",
        "      if str(no) in self.features:\n",
        "        layers.append(x)\n",
        "    return layers"
      ],
      "metadata": {
        "id": "27oiUM7lfqSF"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image"
      ],
      "metadata": {
        "id": "I9Gtkb2Kg1id"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(imgn):\n",
        "  img=Image.open(imgn)\n",
        "  img=loader(img).unsqueeze(0)\n",
        "  return img.to(device)"
      ],
      "metadata": {
        "id": "vABYKFdfhPWv"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "pIxfJkd-hdbK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_size=356"
      ],
      "metadata": {
        "id": "t7TJxmmEhm1s"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as tfm"
      ],
      "metadata": {
        "id": "K3pHxBoHht3G"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader=tfm.Compose(\n",
        "    [\n",
        "        tfm.Resize((img_size,img_size)),\n",
        "        tfm.ToTensor(),\n",
        "        tfm.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.255])\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "OKOoSaoehqXh"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "org_img=load('/content/WhatsApp Image 2025-09-14 at 00.40.12_649453d2.jpg')\n",
        "style=load('/content/style7.jpg')\n",
        "gtd=org_img.clone().requires_grad_(True)"
      ],
      "metadata": {
        "id": "V_xbJI_AiApl"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha=0.000001\n",
        "beta=1"
      ],
      "metadata": {
        "id": "PUWfJoTqiivZ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optim=torch.optim.Adam([gtd],lr=0.0001)"
      ],
      "metadata": {
        "id": "KedSoF34o4WD"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import save_image"
      ],
      "metadata": {
        "id": "nss2vX12pG49"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def denorm(t):\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406], device=t.device).view(1,3,1,1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225], device=t.device).view(1,3,1,1)\n",
        "    return t * std + mean"
      ],
      "metadata": {
        "id": "AcynrNlTxxRb"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=5001\n",
        "model=Vgg().to(device).eval()\n",
        "for epoch in range(epochs):\n",
        "  gen_feats=model(gtd)\n",
        "  org_feats=model(org_img)\n",
        "  style_feats=model(style)\n",
        "\n",
        "  style_loss=org_loss=0\n",
        "  for gen_feat,org_feat,style_feat in zip(gen_feats,org_feats,style_feats):\n",
        "    batch,channel,height,width=gen_feat.shape\n",
        "    org_loss+=torch.mean((gen_feat-org_feat)**2)\n",
        "    G=gen_feat.view(channel,height*width).mm(gen_feat.view(channel,height*width).t())\n",
        "    A=style_feat.view(channel,height*width).mm(style_feat.view(channel,height*width).t())\n",
        "    style_loss+=torch.mean((G-A)**2)\n",
        "  tloss=alpha*org_loss+beta*style_loss\n",
        "  optim.zero_grad()\n",
        "  tloss.backward()\n",
        "  optim.step()\n",
        "\n",
        "  if epoch%200==0:\n",
        "    print(tloss)\n",
        "    save_image(denorm(gtd),f'generated{epoch}.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJE65Mq2i3NW",
        "outputId": "5c233a03-9c20-463e-875c-24e324b9a03c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(21587956., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(20553474., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(19582294., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(18671558., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(17815470., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(17011076., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(16255877., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(15545057., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(14873850., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(14242292., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(13648431., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(13087016., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(12557490., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(12056783., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(11583148., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(11135796., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(10713811., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(10315320., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(9937922., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(9579511., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(9241148., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(8919727., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(8613614., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(8322847., device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(8047243.5000, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(7785080., device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3RmE_762rKdd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}